---
title: "ACOTSP 2000: Estimation of parameter importance with Random Forests (Performance)"
author: "Leslie Pérez Cáceres"
output:
  pdf_document: default
  html_notebook: default
  toc: TRUE
---

```{r include=FALSE}
library(irace)
library(randomForest)
library(randomForestExplainer)
library(knitr)
library(kableExtra)
```

In this document we are testing how to use Random Forest to assess importance and 
interactions of parameters based on the data gathered by irace.

For the analysis below we use as example ACOTSP:

- 20 secs cut off time
- 11 parameters
- 200 instances of size 2000
- 5000 experiments for configuration

We use random forest for predicting 

1. configuration performance
1. configuration normalized performance
2. configuration performance quartile

Models are trained using default settings of the package Random Forest, excepting
the number of trees that was set to 300. We have access to the following measures 
that can be considered indicators of importance:

- mean_min_depth: mean depth of the subtree closest to the tree root, where a variable 
is root of the sub tree. 
- no_of_nodes: number of nodes in which the variable was used to split
- mse_increase: increment of prediction mean squared error
- no_of_trees: number of trees in which the variable was used
- times_a_root: number of times a variable was selected as root variable
- accuracy_decrease: measure of the classification accuracy (only for classification models)

For this analysis we use data generated by irace, it is possible to select 
a subset of the data and perform the analysis. The data is imputed and there is
a procedure to analyze importance based on a reference variable and a retraining 
scheme to assess real importance in conditional parameters. The instance is also used
as a predictor in this data. (details in another document)

There are some things that should be investigated regarding the best way to use the
models to asses interaction and importance:

1. Discretising numerical variables for prediction: based on a comment I got
that RF are biased to select numerical variables as split. This will be particularly
interesting and in line with the fact that irace defines a sampling range around
the current value.

2. Adjusting the number of trees and depth of them. My intuition is that smaller more
smaller trees would be more useful in the task of detecting importance. This is because
lower level splits are not as interesting and higher level splits. Also, to be used as a 
post-execution analysis tool the execution time required to build the model depends
on these parameters.

3. How to understand how this importance or interaction is materialized i.e., 
which are the best parameter values ans how these interact. Can we have an heuristic
idea of this interpreting the forest splits?

4. How to interpret instance importance and interaction when using models not predicting 
directly the performance. Can this help detecting heterogeneous sets?

In the following examples the ACOTSP data is used to predict **solution quality similarly**
of how it is used in SMAC and GGA++. The intuition is that in these models
instance is probably the best predictor of the quality due to the different sizes that
compose the instance set and the nature of the configuration objective (tour length).
In this model all irace data execution was used for training. Note that due to the focused n
ature of irace data (given model convergence) there might be contradicting or not consistent 
interactions in the data.


# 1. Configuration performance as dependent variable

The data set used to train this model defines the variable to predict as the raw performance.

We show importance measures ordered by the mean min depth given that the interaction
analysis is based on this measure.

```{r}
load("../model_data/model-acotsp2000-performance.Rdata")
importance_frame = model$importance_frame
important_parameters = model$important_parameters
full_interactions_frame = model$full_interactions_frame
interactions_frame = model$interactions_frame
kable(importance_frame[order(importance_frame[,"mean_min_depth"]),])  %>%  
      kable_styling(latex_options="scale_down") %>%
  kable_styling(latex_options = "HOLD_position")
```

Ordered importance measures: 
```{r}
imp <- model$importance_frame
imp[,"variable"] <- as.character(imp[,"variable"])
ordered_imp <- matrix(NA, nrow=nrow(imp), ncol=ncol(imp)-2)
measures <- colnames(imp)[!(colnames(imp) %in% c("variable", "p_value"))]
colnames(ordered_imp) <- measures
ordered_imp[,"mean_min_depth"] <- imp[,"variable"][order(imp[,"mean_min_depth"])]
ordered_imp[,"no_of_nodes"] <- imp[,"variable"][order(imp[,"no_of_nodes"], decreasing=TRUE)]
ordered_imp[,"mse_increase"] <- imp[,"variable"][order(abs(imp[,"mse_increase"]), decreasing=TRUE)]
ordered_imp[,"node_purity_increase"] <- imp[,"variable"][order(imp[,"node_purity_increase"], decreasing=TRUE)]
ordered_imp[,"no_of_trees"] <- imp[,"variable"][order(imp[,"no_of_trees"], decreasing=TRUE)]
ordered_imp[,"times_a_root"] <- imp[,"variable"][order(imp[,"times_a_root"], decreasing=TRUE)]
kable(ordered_imp) %>%  
      kable_styling(latex_options="scale_down") %>%
  kable_styling(latex_options = "HOLD_position")

```

We can plot two importance measures using the randomForestExplainer package, we choose the to show the 
mean min depth in the x axis and the mse increase in the y axis. Top 7 variables are highlighted.
In this case, since we are not predicting performance the effect of instance variable in model performance must
be interpreted carefully. These plots are interesting given that contrast importance for ranking prediction in 
terms of accuracy (mse_increase) and in terms of early importance the tree more in line with classification
goals (mean min depth).

```{r fig1, out.width = '75%', fig.align = "center"}

suppressWarnings(plot_multi_way_importance(importance_frame, size_measure = "p_value", 
                          y_measure="mse_increase", x_measure="mean_min_depth", 
                          no_of_labels=7))

```

We can also visualize the relationship between importance measures, this could help to
understand which indicator is more suited or can be used as a complement of other:

```{r fig2, out.width = '70%',  fig.align = "center", message=FALSE}
plot_importance_rankings(importance_frame, measures=c("mse_increase", "mean_min_depth", 
                                                      "no_of_nodes", "times_a_root"))
```


We perform an analysis of conditional parameters importance using a filtering and re training strategy 
and apply an irrelevant parameter filter by including a reference parameter. After this process
the most important 5 parameters are detected.

Important parameters:
```{r}
print(important_parameters)
```

Next, we run the interaction analysis only over important parameters.
This is assuming the interactions one cares to detect are related to them, which
might be not entirely correct ans should be evaluated as heuristic.
The importance of interactions is calculated based on the mean min depth indicator
in its conditional version.

Parameter interaction importance:
```{r}
kable(full_interactions_frame)  %>%  
  kable_styling(latex_options="scale_down") %>%
  kable_styling(latex_options = "HOLD_position")

```

For this example we aggregate bidirectional (`param1:param2` and `param2:param2`) interactions, 
this is done given that we assume that the hierarchy of the interaction is not well represented
in the forest and this should be analyzed separately. We also filter interactions using the reference
parameter to remove all not relevant interactions. Once this process is done, the importance 
of most relevant interactions is summarized.

Relevant parameter interactions:
```{r}
kable(interactions_frame) %>% 
  kable_styling(latex_options="scale_down") %>%
  kable_styling(latex_options = "HOLD_position")

```


# 2. Configuration normalized performance as dependent variable

The data set used to train this model defines the variable to predict as the normalized
performance. Performance is normalized as:

```
(performance - min_performance) / (max_performance - min_performance)
```

where `performance` is the performance of a of a configuration in an instance and `min_performance` and `max_performance` are the minimum and maximum performance on an instance used during the execution of irace.

The parameter importance measures: 

```{r}
load("../model_data/model-acotsp2000-nperformance.Rdata")
importance_frame = model$importance_frame
important_parameters = model$important_parameters
full_interactions_frame = model$full_interactions_frame
interactions_frame = model$interactions_frame
kable(importance_frame[order(importance_frame[,"mean_min_depth"]),])  %>%  
      kable_styling(latex_options="scale_down") %>%
  kable_styling(latex_options = "HOLD_position")

```

Ordered importance measures: 

```{r}
imp <- model$importance_frame
imp[,"variable"] <- as.character(imp[,"variable"])
ordered_imp <- matrix(NA, nrow=nrow(imp), ncol=ncol(imp)-2)
measures <- colnames(imp)[!(colnames(imp) %in% c("variable", "p_value"))]
colnames(ordered_imp) <- measures
ordered_imp[,"mean_min_depth"] <- imp[,"variable"][order(imp[,"mean_min_depth"])]
ordered_imp[,"no_of_nodes"] <- imp[,"variable"][order(imp[,"no_of_nodes"], decreasing=TRUE)]
ordered_imp[,"mse_increase"] <- imp[,"variable"][order(abs(imp[,"mse_increase"]), decreasing=TRUE)]
ordered_imp[,"node_purity_increase"] <- imp[,"variable"][order(imp[,"node_purity_increase"], decreasing=TRUE)]
ordered_imp[,"no_of_trees"] <- imp[,"variable"][order(imp[,"no_of_trees"], decreasing=TRUE)]
ordered_imp[,"times_a_root"] <- imp[,"variable"][order(imp[,"times_a_root"], decreasing=TRUE)]
kable(ordered_imp) %>%  
      kable_styling(latex_options="scale_down") %>%
  kable_styling(latex_options = "HOLD_position")

```

We plot importance as above to visualize how the mse increase and mean min depth are 
related:

```{r fig3, out.width = '75%', fig.align = "center"}

suppressWarnings(plot_multi_way_importance(importance_frame, size_measure = "p_value", 
                          y_measure="mse_increase", x_measure="mean_min_depth", 
                          no_of_labels=7))

```

We visualize the relationship between importance measures, this could help to
understand which indicator is more suited or can be used as a complement of other.

```{r fig4, out.width = '70%', fig.align = "center", message=FALSE}
plot_importance_rankings(importance_frame, measures=c("mse_increase", "mean_min_depth", 
                                                      "no_of_nodes", "times_a_root"))
```

Important parameters:
```{r}
print(important_parameters)
```

Parameter interaction importance:
```{r}
kable(full_interactions_frame)  %>% 
  kable_styling(latex_options="scale_down") %>%
  kable_styling(latex_options = "HOLD_position")

```

For this example we aggregate bidirectional (`param1:param2` and `param2:param2`) interactions, 
this is done given that we assume that the hierarchy of the interaction is not well represented
in the forest and this should be analyzed separately. We also filter interactions using the reference
parameter to remove all not relevant interactions. Once this process is done, the importance 
of most relevant interactions is summarized.

Relevant parameter interactions:
```{r}
kable(interactions_frame) %>% 
  kable_styling(latex_options="scale_down") %>%
  kable_styling(latex_options = "HOLD_position")

```

# 3.Configuration performancequantile as dependent variable

In this example we use the performance quartile as the dependent variable when training,
thus the trained random forest predicts the mean performance quartile of a configuration.


Parameter importance:
```{r }
load("../model_data/model-acotsp2000-qperformance.Rdata")
importance_frame = model$importance_frame
full_interactions_frame = model$full_interactions_frame
interactions_frame = model$interactions_frame
important_parameters = model$important_parameters
kable(importance_frame[order(importance_frame[,"mean_min_depth"]),]) %>%
      kable_styling(latex_options="scale_down") %>%
  kable_styling(latex_options = "HOLD_position")

```

Ordered importance measures: 

```{r}
imp <- model$importance_frame
imp[,"variable"] <- as.character(imp[,"variable"])
ordered_imp <- matrix(NA, nrow=nrow(imp), ncol=ncol(imp)-2)
measures <- colnames(imp)[!(colnames(imp) %in% c("variable", "p_value"))]
colnames(ordered_imp) <- measures
ordered_imp[,"mean_min_depth"] <- imp[,"variable"][order(imp[,"mean_min_depth"])]
ordered_imp[,"no_of_nodes"] <- imp[,"variable"][order(imp[,"no_of_nodes"], decreasing=TRUE)]
ordered_imp[,"mse_increase"] <- imp[,"variable"][order(abs(imp[,"mse_increase"]), decreasing=TRUE)]
ordered_imp[,"node_purity_increase"] <- imp[,"variable"][order(imp[,"node_purity_increase"], decreasing=TRUE)]
ordered_imp[,"no_of_trees"] <- imp[,"variable"][order(imp[,"no_of_trees"], decreasing=TRUE)]
ordered_imp[,"times_a_root"] <- imp[,"variable"][order(imp[,"times_a_root"], decreasing=TRUE)]
kable(ordered_imp) %>%  
      kable_styling(latex_options="scale_down") %>%
  kable_styling(latex_options = "HOLD_position")

```

As expected the instance is the most important variable, the other parameters do not 
have the same ranking in the different benchmarks. 

We plot the importance measures  min mean depth and 
the mse increase to observe better this difference. The instance is removed in the 
plot so its possible to see more clearly other parameters. 


```{r fig5,  out.width = '70%', fig.align = "center"}

plot_multi_way_importance(importance_frame[importance_frame[,"variable"]!="instance",], size_measure = "p_value",
                          y_measure="mse_increase", x_measure="mean_min_depth", no_of_labels=7)

```

We visualize the relationship between importance measures, this could help to
understand which indicator is more suited or can be used as a complement of other.

```{r fig6, out.width = '70%', fig.align = "center", message=FALSE}
plot_importance_rankings(importance_frame, measures=c("mse_increase", "mean_min_depth", 
                                                      "no_of_nodes", "times_a_root"))
```

Filtered important parameters:
```{r}
print(important_parameters)

```

We use the mean_min_depth measure as reference given that this measure can
be extended to assess interactions and run the interaction analysis to find the importance
of interactions between the selected important parameters.
This is done by extending the definition of mean_min_depth to be calculated in max subtrees in which one
variable is root.

```{r}
kable(full_interactions_frame)  %>%  
  kable_styling(latex_options="scale_down") %>%
  kable_styling(latex_options = "HOLD_position")

```

We aggregate interactions with their inverse given that for now we are not interested
in the direction of the interaction. Once we aggregate to have bidirectional interactions
and filter dummy interactions, we get a matrix where the relevant importance of
interactions are summarized:

```{r}
kable(interactions_frame)  %>%  
  kable_styling(latex_options="scale_down") %>%
  kable_styling(latex_options = "HOLD_position")

```

